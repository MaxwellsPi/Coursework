{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is wrangle data from a various sources of data. The data is sourced from the twitter user @WeRateDogs and a udacity provided dataset which classifies dogs using a neural network. After data is gathered, assessed, and cleaned it was used to generate three insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used for in this project consisted of three different datasets.\n",
    "\n",
    "**Twitter Archive File:**\n",
    ">This data was in CSV format and was downloaded to the jupyter workspace by clicking on the jupyer icon then clikcing upload. It was then read by using pandas read_csv() function to read the file into a dataframe.\n",
    "\n",
    "**Tweet image prediction file**\n",
    ">This data was in TSV format and was downloaded using python requests and the get function.  The get function returned an object with the data which was then written to a txt file in the workspace.\n",
    "\n",
    "**Tweet_Json text**\n",
    ">This data was in JSON fomat and was imported using a python function that read the text file line-by-line using readline.  Then the data was extracted by using the JSON package, defining the keys that were of interest and appending the data to a list which was then converted to a pandas dataframe.  In this case, this consisted of tweet_id, favorites and retweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assesing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the three sets of data were loaded into dataframes each dataframe was inspected visually and programatticaly. \n",
    "\n",
    "**Visual assesment** consisted of scrolling through csv files in Excel spreadsheet and pandas dataframes.\n",
    "\n",
    "**Programatic assesment** consisted of using pandas methods and functions such as .describe(), .isnull(), .head(), .sample(), .duplicated() and shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most quality issues that were addressed were in the Twitter archive dataframe.  The issues addressed included:\n",
    "- Missing values\n",
    "- Inconsistencies in data elements\n",
    "- Low-interpretability of column names\n",
    "- Conversion of datatype for various columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After gathering, assessing and cleaning the dataframes, they were merged by tweet_id into one master dataframe then saved as a CSV file within the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
